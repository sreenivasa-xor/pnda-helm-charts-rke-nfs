{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka and Spark Streaming Example with OpenMetrics\n",
    "This notebook Showcase How PNDA platform integrates Jupyter Kafka and Spark.\n",
    "\n",
    "Kafka ingest events in [openMetrics](https://openmetrics.io/) format in the \"openmetrics\" topic.\n",
    "\n",
    "This notebook provides a [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) applicaton to:\n",
    "- transform openmetrics String to a Spark Dataframe.\n",
    "- query the openmetrics stream to get averaged values of the timeseries and their last timestamp.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoding openmetrics strings with spark udf\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import datetime\n",
    "from typing import Callable\n",
    "import time\n",
    "\n",
    "@f.udf(t.StructType([\n",
    "    t.StructField(\"metric\", t.StringType(), False),\n",
    "    t.StructField(\"tags\", t.ArrayType(t.StringType()), False),\n",
    "    t.StructField(\"timestamp\", t.TimestampType(), False),\n",
    "    t.StructField(\"value\", t.DoubleType(), False)]))\n",
    "def decode_openmetrics_udf(input:str) -> dict:\n",
    "    [metric,value,timestamp] = input.split()\n",
    "    [metric,tags] = metric.split(\"{\")\n",
    "    tags = tags[:-1].split(\",\")\n",
    "    #return ({ \"metric\": metric,\n",
    "    #         \"tags\": tags,\n",
    "    #         \"timestamp\":  datetime.datetime.fromtimestamp(float(timestamp)/1000),\n",
    "    #         \"value\": float(value) } )\n",
    "    return t.Row('metric','tags','timestamp','value')(metric,\n",
    "                                                      tags,\n",
    "                                                      datetime.datetime.fromtimestamp(float(timestamp)/1000.0),\n",
    "                                                      float(value))\n",
    "def decode_openmetrics(col_input:str, rm_input_df: bool=False) -> Callable[[DataFrame], DataFrame]:\n",
    "    def F(df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn('temp_col', decode_openmetrics_udf(f.col(col_input)))\n",
    "        if rm_input_df:\n",
    "            return  df.select('temp_col.*')\n",
    "        else:\n",
    "            df_columns = df.columns\n",
    "            df_columns.append('temp_col.*')\n",
    "            return df.select(df_columns).drop('temp_col')\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Kafka and Spark\n",
    "Kafka is accessible from notebooks through the \"pnda-cp-kafka\" hostname.\n",
    "\n",
    "PNDA deploys a K8s service with that name pointing to its kafka brokers.\n",
    "\n",
    "Jupyter's PySpark kernel points to \"spark://pnda-spark-standalone:7077\" by default, the PNDA spark-standalone component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"pnda-cp-kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"openmetrics\") \\\n",
    "  .load().selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING) as input_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openmetrics_stream = input_ds.transform(decode_openmetrics(\"input_value\")).writeStream.queryName(\"openmetrics\").format(\"memory\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Query with SQL\n",
    "Each 5 seconds we query the kafka stream through Spark Structured Streaming to get timeseries average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print ('metrics average at {}'.format(datetime.datetime.now()))\n",
    "    spark.sql(\"select metric, tags, avg(value), max(timestamp) as last_seen from openmetrics group by metric, tags\").show(10, False)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openmetrics_stream.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
